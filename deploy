#!/bin/bash

set -e;

# Load environment variables from .env.local if it exists
if [ -f .env.local ]; then
  source .env.local
fi

# Parse arguments
# Use ADMIN_PROFILE for deployments if available (has CloudFormation permissions)
# Otherwise fall back to AWS_PROFILE
CDK_PROFILE=${ADMIN_PROFILE:-${AWS_PROFILE:-default}}
CDK_REGION=${AWS_REGION:-us-west-2}
DEPLOYMENT_ENV=${NODE_ENV:-development}

# Export CDK_DEFAULT_ACCOUNT from AWS_ACCOUNT_ID if set
# This ensures CDK uses the correct account ID from .env.local
if [ ! -z "$AWS_ACCOUNT_ID" ]; then
  export CDK_DEFAULT_ACCOUNT=$AWS_ACCOUNT_ID
  echo "Using Account ID from .env.local: $AWS_ACCOUNT_ID"
fi

# Export CDK_DEFAULT_REGION
export CDK_DEFAULT_REGION=$CDK_REGION
SKIP_UNIT_TESTS=false
SKIP_INTEGRATION_TESTS=false
SKIP_E2E=false
while [[ "$#" -gt 0 ]]; do case $1 in
  --profile) CDK_PROFILE="$2"; shift;;
  --region) AWS_REGION="$2"; shift;;
  --env) DEPLOYMENT_ENV="$2"; shift;;
  --skip-unit-tests) SKIP_UNIT_TESTS=true;;
  --skip-integration-tests) SKIP_INTEGRATION_TESTS=true;;
  --skip-e2e) SKIP_E2E=true;;
  --skip-tests) SKIP_UNIT_TESTS=true; SKIP_INTEGRATION_TESTS=true; SKIP_E2E=true;;
  --no-test) SKIP_UNIT_TESTS=true; SKIP_INTEGRATION_TESTS=true; SKIP_E2E=true;;
esac; shift; done

# Check if region is set
if [ -z "$AWS_REGION" ]; then
  # Try to get region from AWS config
  AWS_REGION=$(aws configure get region --profile $CDK_PROFILE 2>/dev/null)
  
  if [ -z "$AWS_REGION" ]; then
    echo "Error: AWS region is required"
    echo "Usage: ./deploy [--profile <aws_profile>] [--region <aws_region>] [--env <environment>] [--skip-unit-tests] [--skip-integration-tests] [--skip-e2e] [--skip-tests] [--no-test]"
    echo "Example: ./deploy --profile dev --region us-west-2 --env development"
    echo "         ./deploy --skip-unit-tests  # Skip unit tests before deployment"
    echo "         ./deploy --skip-integration-tests  # Skip integration tests after deployment"
    echo "         ./deploy --skip-e2e  # Skip Phase 4 E2E test after deployment"
    echo "         ./deploy --skip-tests  # Skip all tests (unit, integration, E2E)"
    echo "You may also configure your region by running 'aws configure'"
    exit 1
  fi
fi

echo "Using AWS Profile: $CDK_PROFILE"
echo "Using AWS Region: $AWS_REGION"
echo "Using Environment: $DEPLOYMENT_ENV"

# Validate required Bedrock configuration
if [ -z "$BEDROCK_MODEL" ]; then
  echo "Error: BEDROCK_MODEL is required in .env.local"
  exit 1
fi

# Build and deploy infrastructure
echo "Building and deploying infrastructure..."
# Skip npm ci if node_modules exists and build works (faster, avoids permission issues)
if [ ! -d "node_modules" ] || ! npm run build > /dev/null 2>&1; then
  echo "Installing dependencies..."
  npm install
fi
npm run build

# Run unit tests before deployment (unless skipped)
if [ "$SKIP_UNIT_TESTS" = false ]; then
  echo ""
  echo "üß™ Running unit tests..."
  if npm test; then
    echo "‚úÖ All unit tests passed"
  else
    echo "‚ùå Unit tests failed - deployment aborted"
    echo "   Use --skip-unit-tests to deploy without running unit tests"
    exit 1
  fi
else
  echo ""
  echo "‚è≠Ô∏è  Skipping unit tests"
fi

# Prepare CDK context parameters
CDK_CONTEXT_PARAMS=""
CDK_CONTEXT_PARAMS="$CDK_CONTEXT_PARAMS -c nodeEnv=$DEPLOYMENT_ENV"

# Set logging level (default to info)
LOG_LEVEL=${LOG_LEVEL:-info}
CDK_CONTEXT_PARAMS="$CDK_CONTEXT_PARAMS -c logLevel=$LOG_LEVEL"

# Pass Bedrock and AWS configuration as CDK context parameters
CDK_CONTEXT_PARAMS="$CDK_CONTEXT_PARAMS -c bedrockModel=$BEDROCK_MODEL"
CDK_CONTEXT_PARAMS="$CDK_CONTEXT_PARAMS -c awsRegion=$AWS_REGION"

echo "Using Bedrock Model: $BEDROCK_MODEL"

# Set defaults for S3 Buckets (optional - from .env.local)
EVIDENCE_LEDGER_BUCKET=${EVIDENCE_LEDGER_BUCKET:-}
WORLD_STATE_SNAPSHOTS_BUCKET=${WORLD_STATE_SNAPSHOTS_BUCKET:-}
SCHEMA_REGISTRY_BUCKET=${SCHEMA_REGISTRY_BUCKET:-}
ARTIFACTS_BUCKET=${ARTIFACTS_BUCKET:-}
LEDGER_ARCHIVES_BUCKET=${LEDGER_ARCHIVES_BUCKET:-}

# Pass S3 bucket names to CDK if provided
if [ ! -z "$EVIDENCE_LEDGER_BUCKET" ]; then
  CDK_CONTEXT_PARAMS="$CDK_CONTEXT_PARAMS -c evidenceLedgerBucket=$EVIDENCE_LEDGER_BUCKET"
  echo "Using Evidence Ledger Bucket: $EVIDENCE_LEDGER_BUCKET"
fi

if [ ! -z "$WORLD_STATE_SNAPSHOTS_BUCKET" ]; then
  CDK_CONTEXT_PARAMS="$CDK_CONTEXT_PARAMS -c worldStateSnapshotsBucket=$WORLD_STATE_SNAPSHOTS_BUCKET"
  echo "Using World State Snapshots Bucket: $WORLD_STATE_SNAPSHOTS_BUCKET"
fi

if [ ! -z "$SCHEMA_REGISTRY_BUCKET" ]; then
  CDK_CONTEXT_PARAMS="$CDK_CONTEXT_PARAMS -c schemaRegistryBucket=$SCHEMA_REGISTRY_BUCKET"
  echo "Using Schema Registry Bucket: $SCHEMA_REGISTRY_BUCKET"
fi

if [ ! -z "$ARTIFACTS_BUCKET" ]; then
  CDK_CONTEXT_PARAMS="$CDK_CONTEXT_PARAMS -c artifactsBucket=$ARTIFACTS_BUCKET"
  echo "Using Artifacts Bucket: $ARTIFACTS_BUCKET"
fi

if [ ! -z "$LEDGER_ARCHIVES_BUCKET" ]; then
  CDK_CONTEXT_PARAMS="$CDK_CONTEXT_PARAMS -c ledgerArchivesBucket=$LEDGER_ARCHIVES_BUCKET"
  echo "Using Ledger Archives Bucket: $LEDGER_ARCHIVES_BUCKET"
fi

echo "Using Log Level: $LOG_LEVEL"

# Look up DynamoDB managed prefix list ID for this region (Internal Adapter VPC egress)
DYNAMODB_PREFIX_LIST_ID=$(aws ec2 describe-managed-prefix-lists \
  --profile $CDK_PROFILE \
  --region $AWS_REGION \
  --no-cli-pager \
  --filters "Name=prefix-list-name,Values=com.amazonaws.$AWS_REGION.dynamodb" \
  --query 'PrefixLists[0].PrefixListId' \
  --output text) || DYNAMODB_PREFIX_LIST_ID=""
if [ -n "$DYNAMODB_PREFIX_LIST_ID" ]; then
  CDK_CONTEXT_PARAMS="$CDK_CONTEXT_PARAMS -c dynamoDbPrefixListId=$DYNAMODB_PREFIX_LIST_ID"
  echo "Using DynamoDB prefix list (VPC): $DYNAMODB_PREFIX_LIST_ID"
else
  echo "Warning: Could not look up DynamoDB prefix list for $AWS_REGION; deploy may fail if Internal Adapter is in VPC"
fi

# Find existing stack
echo "Finding existing CC Native stack..."
STACK_NAME=$(aws cloudformation list-stacks \
  --profile $CDK_PROFILE \
  --region $AWS_REGION \
  --no-cli-pager \
  --stack-status-filter CREATE_COMPLETE UPDATE_COMPLETE UPDATE_ROLLBACK_COMPLETE \
  --query "StackSummaries[?starts_with(StackName, 'CCNativeStack')].StackName" \
  --output text | head -n 1)

if [ -z "$STACK_NAME" ]; then
  echo "No existing CC Native stack found. Creating new stack..."
  npx cdk --profile $CDK_PROFILE --region $AWS_REGION deploy CCNativeStack --require-approval never $CDK_CONTEXT_PARAMS
else
  echo "Updating existing stack: $STACK_NAME"
  npx cdk --profile $CDK_PROFILE --region $AWS_REGION deploy CCNativeStack --require-approval never $CDK_CONTEXT_PARAMS
fi

# Get stack outputs
echo "Getting stack outputs..."
STACK_OUTPUTS=$(aws cloudformation describe-stacks \
  --profile $CDK_PROFILE \
  --region $AWS_REGION \
  --no-cli-pager \
  --stack-name CCNativeStack \
  --query 'Stacks[0].Outputs' \
  --output json)

# Extract output values (use stack outputs, or fall back to .env.local if provided)
EVIDENCE_LEDGER_BUCKET_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="EvidenceLedgerBucketName") | .OutputValue')
WORLD_STATE_SNAPSHOTS_BUCKET_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="WorldStateSnapshotsBucketName") | .OutputValue')
SCHEMA_REGISTRY_BUCKET_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="SchemaRegistryBucketName") | .OutputValue')
ARTIFACTS_BUCKET_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="ArtifactsBucketName") | .OutputValue')
LEDGER_ARCHIVES_BUCKET_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="LedgerArchivesBucketName") | .OutputValue')
EVENT_BUS_NAME=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="EventBusName") | .OutputValue')

# Extract DynamoDB table names from stack outputs
WORLD_STATE_TABLE_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="WorldStateTableName") | .OutputValue')
EVIDENCE_INDEX_TABLE_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="EvidenceIndexTableName") | .OutputValue')
SNAPSHOTS_INDEX_TABLE_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="SnapshotsIndexTableName") | .OutputValue')
SCHEMA_REGISTRY_TABLE_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="SchemaRegistryTableName") | .OutputValue')
CRITICAL_FIELD_REGISTRY_TABLE_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="CriticalFieldRegistryTableName") | .OutputValue')
LEDGER_TABLE_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="LedgerTableName") | .OutputValue')
CACHE_TABLE_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="CacheTableName") | .OutputValue')
TENANTS_TABLE_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="TenantsTableName") | .OutputValue')
ACCOUNTS_TABLE_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="AccountsTableName") | .OutputValue')
SIGNALS_TABLE_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="SignalsTableName") | .OutputValue')
METHODOLOGY_TABLE_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="MethodologyTableName") | .OutputValue')
ASSESSMENT_TABLE_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="AssessmentTableName") | .OutputValue')
IDENTITIES_TABLE_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="IdentitiesTableName") | .OutputValue')
ACCOUNT_POSTURE_STATE_TABLE_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="AccountPostureStateTableName") | .OutputValue')
GRAPH_MATERIALIZATION_STATUS_TABLE_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="GraphMaterializationStatusTableName") | .OutputValue')
NEPTUNE_CLUSTER_ENDPOINT_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="NeptuneClusterEndpoint") | .OutputValue')
NEPTUNE_CLUSTER_PORT_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="NeptuneClusterPort") | .OutputValue')
VPC_ID_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="VpcId") | .OutputValue')
NEPTUNE_SUBNET_IDS_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="NeptuneSubnetIds") | .OutputValue')
NEPTUNE_SUBNET_ID_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="NeptuneSubnetId") | .OutputValue')
AGENT_ROLE_ARN_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="AgentRoleArn") | .OutputValue')
USER_POOL_ID_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="UserPoolId") | .OutputValue')
USER_POOL_CLIENT_ID_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="UserPoolClientId") | .OutputValue')
DECISION_API_URL_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="DecisionApiUrl") | .OutputValue')
DECISION_API_KEY_ID_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="DecisionApiKeyId") | .OutputValue')

# Phase 4: Execution Infrastructure table outputs
ACTION_TYPE_REGISTRY_TABLE_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="ActionTypeRegistryTableName") | .OutputValue')
ACTION_INTENT_TABLE_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="ActionIntentTableName") | .OutputValue')
EXECUTION_ATTEMPTS_TABLE_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="ExecutionAttemptsTableName") | .OutputValue')
EXECUTION_OUTCOMES_TABLE_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="ExecutionOutcomesTableName") | .OutputValue')
EXTERNAL_WRITE_DEDUPE_TABLE_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="ExternalWriteDedupeTableName") | .OutputValue')
CONNECTOR_CONFIG_TABLE_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="ConnectorConfigTableName") | .OutputValue')
INTERNAL_NOTES_TABLE_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="InternalNotesTableName") | .OutputValue')
INTERNAL_TASKS_TABLE_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="InternalTasksTableName") | .OutputValue')

# Phase 5.2: Decision Scheduling table outputs
DECISION_RUN_STATE_TABLE_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="DecisionRunStateTableName") | .OutputValue')
IDEMPOTENCY_STORE_TABLE_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="IdempotencyStoreTableName") | .OutputValue')

# Phase 5.3: Perception Scheduler table outputs
PERCEPTION_SCHEDULER_TABLE_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="PerceptionSchedulerTableName") | .OutputValue')
PULL_IDEMPOTENCY_STORE_TABLE_OUTPUT=$(echo "$STACK_OUTPUTS" | jq -r '.[] | select(.OutputKey=="PullIdempotencyStoreTableName") | .OutputValue')

# Retrieve API key value (required for API calls - API Gateway needs the actual key value, not just ID)
if [ -n "$DECISION_API_KEY_ID_OUTPUT" ] && [ "$DECISION_API_KEY_ID_OUTPUT" != "null" ]; then
  echo "Retrieving API key value..."
  DECISION_API_KEY_VALUE=$(aws apigateway get-api-key \
    --api-key "$DECISION_API_KEY_ID_OUTPUT" \
    --include-value \
    --region "$AWS_REGION" \
    --query 'value' \
    --output text \
    --profile $CDK_PROFILE \
    --no-cli-pager 2>/dev/null || echo "")
  
  if [ -z "$DECISION_API_KEY_VALUE" ]; then
    echo "‚ö†Ô∏è  Warning: Could not retrieve API key value. You may need to retrieve it manually."
  fi
else
  DECISION_API_KEY_VALUE=""
fi

# Use stack outputs (preferred) or fall back to .env.local values
EVIDENCE_LEDGER_BUCKET=${EVIDENCE_LEDGER_BUCKET_OUTPUT:-$EVIDENCE_LEDGER_BUCKET}
WORLD_STATE_SNAPSHOTS_BUCKET=${WORLD_STATE_SNAPSHOTS_BUCKET_OUTPUT:-$WORLD_STATE_SNAPSHOTS_BUCKET}
SCHEMA_REGISTRY_BUCKET=${SCHEMA_REGISTRY_BUCKET_OUTPUT:-$SCHEMA_REGISTRY_BUCKET}
ARTIFACTS_BUCKET=${ARTIFACTS_BUCKET_OUTPUT:-$ARTIFACTS_BUCKET}
LEDGER_ARCHIVES_BUCKET=${LEDGER_ARCHIVES_BUCKET_OUTPUT:-$LEDGER_ARCHIVES_BUCKET}

# Create .env file with stack outputs
cat > .env << EOF
# Autonomous Revenue Decision Loop Configuration
# Generated by deploy script on $(date)

# AWS Configuration
AWS_REGION=$AWS_REGION
AWS_PROFILE=$CDK_PROFILE

# Environment
NODE_ENV=$DEPLOYMENT_ENV

# S3 Buckets (World Model)
EVIDENCE_LEDGER_BUCKET=$EVIDENCE_LEDGER_BUCKET
WORLD_STATE_SNAPSHOTS_BUCKET=$WORLD_STATE_SNAPSHOTS_BUCKET
SCHEMA_REGISTRY_BUCKET=$SCHEMA_REGISTRY_BUCKET
ARTIFACTS_BUCKET=$ARTIFACTS_BUCKET
LEDGER_ARCHIVES_BUCKET=$LEDGER_ARCHIVES_BUCKET

# EventBridge
EVENT_BUS_NAME=$EVENT_BUS_NAME

# DynamoDB Tables (World Model)
WORLD_STATE_TABLE_NAME=$WORLD_STATE_TABLE_OUTPUT
EVIDENCE_INDEX_TABLE_NAME=$EVIDENCE_INDEX_TABLE_OUTPUT
SNAPSHOTS_INDEX_TABLE_NAME=$SNAPSHOTS_INDEX_TABLE_OUTPUT
SCHEMA_REGISTRY_TABLE_NAME=$SCHEMA_REGISTRY_TABLE_OUTPUT
CRITICAL_FIELD_REGISTRY_TABLE_NAME=$CRITICAL_FIELD_REGISTRY_TABLE_OUTPUT

# DynamoDB Tables (Application)
LEDGER_TABLE_NAME=$LEDGER_TABLE_OUTPUT
CACHE_TABLE_NAME=$CACHE_TABLE_OUTPUT
TENANTS_TABLE_NAME=$TENANTS_TABLE_OUTPUT
ACCOUNTS_TABLE_NAME=$ACCOUNTS_TABLE_OUTPUT
SIGNALS_TABLE_NAME=$SIGNALS_TABLE_OUTPUT

# DynamoDB Tables (Methodology)
METHODOLOGY_TABLE_NAME=$METHODOLOGY_TABLE_OUTPUT
ASSESSMENT_TABLE_NAME=$ASSESSMENT_TABLE_OUTPUT

# DynamoDB Tables (Identity)
IDENTITIES_TABLE_NAME=$IDENTITIES_TABLE_OUTPUT

# DynamoDB Tables (Phase 2)
ACCOUNT_POSTURE_STATE_TABLE_NAME=$ACCOUNT_POSTURE_STATE_TABLE_OUTPUT
GRAPH_MATERIALIZATION_STATUS_TABLE_NAME=$GRAPH_MATERIALIZATION_STATUS_TABLE_OUTPUT

# DynamoDB Tables (Phase 4: Execution Infrastructure)
ACTION_TYPE_REGISTRY_TABLE_NAME=$ACTION_TYPE_REGISTRY_TABLE_OUTPUT
ACTION_INTENT_TABLE_NAME=$ACTION_INTENT_TABLE_OUTPUT
EXECUTION_ATTEMPTS_TABLE_NAME=$EXECUTION_ATTEMPTS_TABLE_OUTPUT
EXECUTION_OUTCOMES_TABLE_NAME=$EXECUTION_OUTCOMES_TABLE_OUTPUT
EXTERNAL_WRITE_DEDUPE_TABLE_NAME=$EXTERNAL_WRITE_DEDUPE_TABLE_OUTPUT
CONNECTOR_CONFIG_TABLE_NAME=$CONNECTOR_CONFIG_TABLE_OUTPUT
INTERNAL_NOTES_TABLE_NAME=$INTERNAL_NOTES_TABLE_OUTPUT
INTERNAL_TASKS_TABLE_NAME=$INTERNAL_TASKS_TABLE_OUTPUT

# DynamoDB Tables (Phase 5.2: Decision Scheduling)
DECISION_RUN_STATE_TABLE_NAME=$DECISION_RUN_STATE_TABLE_OUTPUT
IDEMPOTENCY_STORE_TABLE_NAME=$IDEMPOTENCY_STORE_TABLE_OUTPUT

# DynamoDB Tables (Phase 5.3: Perception Scheduler)
PERCEPTION_SCHEDULER_TABLE_NAME=$PERCEPTION_SCHEDULER_TABLE_OUTPUT
PULL_IDEMPOTENCY_STORE_TABLE_NAME=$PULL_IDEMPOTENCY_STORE_TABLE_OUTPUT

# Neptune (Phase 2)
NEPTUNE_CLUSTER_ENDPOINT=$NEPTUNE_CLUSTER_ENDPOINT_OUTPUT
NEPTUNE_CLUSTER_PORT=$NEPTUNE_CLUSTER_PORT_OUTPUT

# VPC Configuration (for test runner setup)
VPC_ID=$VPC_ID_OUTPUT
NEPTUNE_SUBNET_IDS=$NEPTUNE_SUBNET_IDS_OUTPUT
NEPTUNE_SUBNET_ID=$NEPTUNE_SUBNET_ID_OUTPUT

# IAM Roles
AGENT_ROLE_ARN=$AGENT_ROLE_ARN_OUTPUT

# Cognito
USER_POOL_ID=$USER_POOL_ID_OUTPUT
USER_POOL_CLIENT_ID=$USER_POOL_CLIENT_ID_OUTPUT

# Phase 3: Decision API
DECISION_API_URL=$DECISION_API_URL_OUTPUT
DECISION_API_KEY_ID=$DECISION_API_KEY_ID_OUTPUT
DECISION_API_KEY=$DECISION_API_KEY_VALUE

# Logging
LOG_LEVEL=$LOG_LEVEL
EOF

echo "Environment variables saved to .env file"

# Run master seed script (intelligently seeds all required data)
echo ""
echo "üå± Running master seed script..."
if npm run seed:all 2>&1; then
  echo "‚úÖ Seed script completed successfully"
else
  echo "‚ùå Seed script failed - required tables may be missing"
  echo "   Please ensure all infrastructure is deployed before seeding"
  exit 1
fi

# Run integration tests after deployment (uses .env written above)
if [ "$SKIP_INTEGRATION_TESTS" = false ]; then
  echo ""
  echo "üß™ Running integration tests (post-deploy)..."
  if npm run test:integration; then
    echo "‚úÖ Integration tests passed"
  else
    echo "‚ùå Integration tests failed - check .env and deployed resources"
    echo "   Use --skip-integration-tests to deploy without running integration tests"
    exit 1
  fi
else
  echo ""
  echo "‚è≠Ô∏è  Skipping integration tests"
fi

# Run Phase 4 E2E test (one deterministic path: seed -> EventBridge -> verify)
if [ "$SKIP_E2E" = false ]; then
  echo ""
  echo "üß™ Running Phase 4 E2E test (seed -> execution -> verify -> cleanup)..."
  chmod +x scripts/phase_4/test-phase4-execution.sh scripts/phase_4/seed-phase4-e2e-intent.sh 2>/dev/null || true
  if ./scripts/phase_4/test-phase4-execution.sh; then
    echo "‚úÖ Phase 4 E2E test passed"
  else
    echo "‚ùå Phase 4 E2E test failed - check .env (EVENT_BUS_NAME, execution tables), EventBridge rule, and Step Functions"
    echo "   Use --skip-e2e to complete deploy without running E2E"
    exit 1
  fi
else
  echo ""
  echo "‚è≠Ô∏è  Skipping Phase 4 E2E test"
fi

echo ""
echo "üéâ Autonomous Revenue Decision Loop deployment complete!"
echo ""
echo "üìä Stack: CCNativeStack"
echo "üåê Region: $AWS_REGION"
echo "üîß Environment: $DEPLOYMENT_ENV"
echo ""
echo "üì¶ S3 Buckets (World Model):"
echo "   Evidence Ledger: $EVIDENCE_LEDGER_BUCKET"
echo "   World State Snapshots: $WORLD_STATE_SNAPSHOTS_BUCKET"
echo "   Schema Registry: $SCHEMA_REGISTRY_BUCKET"
echo "   Artifacts: $ARTIFACTS_BUCKET"
echo "   Ledger Archives: $LEDGER_ARCHIVES_BUCKET"
echo ""
echo "üì° EventBridge:"
echo "   Event Bus: $EVENT_BUS_NAME"
echo ""
echo "üìã DynamoDB Tables (configured in CDK):"
echo "   - cc-native-accounts"
echo "   - cc-native-signals"
echo "   - cc-native-tool-runs"
echo "   - cc-native-approval-requests"
echo "   - cc-native-action-queue"
echo "   - cc-native-policy-config"
echo "   - cc-native-ledger"
echo "   - cc-native-cache"
echo "   - cc-native-tenants"
echo "   - cc-native-world-state"
echo "   - cc-native-evidence-index"
echo "   - cc-native-snapshots-index"
echo "   - cc-native-schema-registry"
echo "   - cc-native-critical-field-registry"
echo "   - cc-native-methodology"
echo "   - cc-native-assessment"
echo ""
echo "üìù Configuration saved to .env file"
echo ""
echo "üß™ Next Steps:"
echo "   1. Review .env file for configuration"
echo "   2. Run integration tests anytime: npm run test:integration"
echo "   3. Run Phase 4 E2E anytime: ./scripts/phase_4/test-phase4-execution.sh"
echo ""
